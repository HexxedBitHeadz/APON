### **Checks**

- [ ] Login portals (More in '**Login portals'** below**)**
    - [ ] Try the default credentials off the application
        - [ ] Admin / admin
        - [ ] Administrator / administrator
        - [ ] root
        - [ ] CMS Name
        - [ ] Repeat user as pass (`john`:`john`)
        - [ ] Username + CMS (ex: username: `administratorjoomla`, password: `cactiuser`)
        - [ ] Repeat one of the above 2 times (ex: username: `adminadmin`, password: `adminadmin`)
        - [ ] Repeat one of the above 3 times (ex: username: `adminadminadmin`, password: `cacticacticacti`)
        - [ ] Repeat one of the above + numbers (ex: username: `admin123`, password: `cacti123`)
        - [ ] CMS + user (ex: `cactiuser` or `usercacti`)
        - [ ] try admin:`cmsname123`
    - [ ] Try usernames already seen throughout the application or in other services like SMTP
    - [ ] Try SQL injection bypasses `1' or 1 = 1-- -` (Check in '**Login portals'** below**)**
    - [ ] Try registering a new user
    - [ ] Brute force with hydra, medusa
- [ ] Brute force directories to find hidden content
- [ ] Check for passwords/URLs/versions/ in the source (things like comments of web app, Developer Remarks, passwords or any weird code)
- [ ] Check version numbers for known exploits
    - [ ] Check changelog for version information
    - [ ] Estimate version based on copyright date (if not automatically adjusted)
- [ ] Check if specific CMS is used like WordPress and then use platform-specific scanners
- [ ] Ways to RCE:
    - [ ] Check for file upload functionalities (if uploads are filtered, try alternative extensions)
    - [ ] Execute commands through SQLi
    - [ ] Shellshock
    - [ ] Command injection
    - [ ] Trigger injected code through path traversal


### <u> Web Enumeration - For every http(s) service you find! </u>
| Response Code | Description |
| ----------- | ----------- |
| 200s | Successes |
| 300s | Redirects |
| 400s | Failed access |
| 500s | Error |
---
- [ ] Searchsploit - "simple file list", not "simple-file-list"
- [ ] Googling everything for exploits!
	- [ ] "Framework" exploit
	- [ ]  "Framework" remote code execution
	- [ ] "Framework" reverse shell
- [ ] Source code (HTTrack) - remember Openemr?
==When looking for exploits online, try to find a few exploit to try before diving in and getting stuck on one!==
---
#### Nikto and Uniscan
![[Nikto Uniscan#Commands]]
- [ ] **Nikto and Uniscan**
#### robots and security
- [ ] Check for <u>/robots.txt</u>
- [ ] Check for <u>/security.txt</u>
Use BOTH, dirsearch and feroxbuster.  This section will help the scan results of both tools, combine them into 1 file, sort and unique a final list.
ROADMAP:
1. Parse OPTIONS headers (looking for PUT/Delete, etc).
2. Download all URL findings, attempt to "auto-grep" out findings, such as html comments, keywords like username, password, etc.
#### 1. Dirsearch & Feroxbuster
**Dirsearch**
- [ ] [[dirsearch]]

**Feroxbuster**

- [ ] [[Feroxbuster]]

#### 2. Check for /cgi-bin/
If cgi-bin directory is found, re-run dirsearch and feroxbuster at `$TARGET:80/cgi-bin/` with extensions of `cgi`, `sh`, `pl` and `py`.  Also check for [[9._Shell shock]]
#### 3. Combining Dirsearch and Feroxbuster results
Combine Dirsearch and Feroxbuster lists:
```bash - kali
cat feroxbuster-COMMON.txt | tr -s " " | cut -d " " -f 6 | grep http > dirList & cat dirsearch-COMMON.txt | tr -s " " | cut -d " " -f 3 | grep http >> dirList
```
#### Python crawler
[[python crawler]]
```bash - kali
python crawl.py dirList
```
sort / unique results, save to same file:
```bash - kali
sort -u -o dirList{,}
```
To import findings into Burp / Zap (make sure tool is running!):
```bash - kali
for u in $URLS=$(cat dirList); do curl $u --proxy http://localhost:8080; done
```
Now check Burp / Zap, should see urls travel through!
Get headers (needed?):
```bash - kali
for u in $URLS=$(cat dirList | tr -s " " | cut -d " " -f 6); do echo $u & curl -I $u; done
```
Check for Options(Not complete):
```bash - kali
for u in $URLS=$(cat dirList | tr -s " " | cut -d " " -f 6); do echo $u * curl -v -X OPTIONS $u; done
```
#### Creating a password list with cewl
[[Cewl#dirList password creator]]
#### Sub directory
```
wfuzz -c -f subs.txt -w /usr/share/seclists/Discovery/DNS/dns-Jhaddix.txt -u "http://$TARGET/" -H "Host: FUZZ.$TARGET" --hl 8
```
#### Login Page
**I found a login page, now what?**
[[2._Login Forms]]
- [ ] Checked against login page
- [ ] [[Hydra]]
- [ ] [[Patator]]
#### Put Header
[[8._PUT Header]]
- [ ] **put header**
#### Copy source code & Grep
[[wget#Download web pages w/ layers]]
* **WGET**
[[WebHTTrack]]
- [ ] **WEBHTTRACK**
[[HTTrack]]
- [ ] **HTTRACK**
[[Grep]]
- [ ] **Grep'd for goodies**
#### HTTP Verb Change
- [ ] **Change HTTP Verb**
In this form, we see a lot of "GET" requests, however if we press the buttons or try to visit the URLs, we get "Cannot Connect"
```
1 <h1>DevOps Dashboard</h1>
<hr>
<form action='http://169.254.109.39:33333/list-current-deployments' method='GET'>
4 <input type='submit' value='List Current Deployments'>
5 </form>
6 <br>
7 <form action='http://169.254.109.39:33333/list-running-procs method='GET'>
8 <input type='submit' value='List Running Processes'>
9</form>
Lo<br>
<form action='http://169.254.109.39:33333/list-active-nodes' method='GET'>
12 <input type='submit' value='List Active Nodes'>
3 </form>
4 <hr>
```
```
Unable to connect
Firefox can't establish a connection to the server at 169.254.109.39:33333.
• The site could be temporarily unavailable or too busy. Try again in a few moments.
• If you are unable to load any pages, check your computer's network connection.
• If your computer or network is protected by a firewall or proxy, make sure that Firefox is permitted
to access the Web.
Try Again
```
So what we can try is to change the HTTP verb with curl as shown below:
![[Curl#Edit Method]]
#### Cookies
[[2._Cookies]]
- [ ] **Cookie Privileges**
#### Hidden Form
- [ ] **Hidden Form**
[[2._Hidden Form]]
#### Commix
https://www.kali.org/tools/commix/
```bash - kali
commix --url="http://$TARGET/" --level 3
```
#### All else false, try =>
[[0._TCM Web Apps]]
---
- [ ] Whatweb review
Collect ANY framework names that we can search exploit for:
- [ ] /robots.txt
- [ ] Wappalyzer review
- [ ] CMSEEK
- [ ] File upload - Webshell, exe, elf, xxe, etc
- [ ] Searchsploit / GitHub
			==CMS - SEARCHCSPLOIT EVERYTHING [Simple College Website :( ]==
- [ ] SQL injection testing [[1_SQL Injection]]
- [ ] LFI testing [[3._LFI RFI]]
- [ ] Burpsuite Spider  1_7_36 https://portswigger.net/burp/releases/professional-community-1-7-36
- [ ] FUZZing [[wfuzz]]
- [ ] Check for [[10._JavaScirpt]]
- [ ] VHOST check with [[ffuf#VHOST discovery]] or [[Gobuster#VHOST discovery]]
- [ ] wafw00f  he Web Application Firewall Fingerprinting Toolkit
- [ ] python3 ReconSpider.py <URL>


### Methodologies
Usually I approach like this:-
1.whatweb
2.ffuf with php,aspx,etc extension
3.nitko
4.check source code.
5.spidering using owaspzap / old burb
6.chcek if any SQLi exists on page.
7.check if any CMS on page
---
Hi! I can give you a couple of suggestions and some steps to add to your enumeration phase, based on my experience on HackTheBox/VulnHub Machines.
 - [ ] If the website has an hostname, bruteforce subdomains (I personally use gobuster for this).
 - [ ] If you find interesting files (like "config.php" or "upload.php" or "panel.php") but you can't access to them, fuzz for their backup's files (config.old, config.bak, config.php.old, config.php.bak ...)
 - [ ] If SQL Auth Bypass doesn't work, spend a couple of minutes to test for a NoSQL Auth Bypass (you never know :)
 - [ ] If you can access to weird directories, that you think are not properly configured, check for http-verbs (you always want PUT enabled :)
---
